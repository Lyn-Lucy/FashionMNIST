{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install d2l","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-30T06:20:42.346429Z","iopub.execute_input":"2022-03-30T06:20:42.346978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom torch import nn\nfrom d2l import torch as d2l\nimport time\n\nstart = time.perf_counter()\n\ntrain_augs = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(224),\n    torchvision.transforms.RandomHorizontalFlip(),\n    torchvision.transforms.ToTensor(),\n    transforms.Normalize(mean = (0.5), std = (0.5))])\n\ntest_augs = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(224),\n    torchvision.transforms.ToTensor(),\n    transforms.Normalize(mean = (0.5), std = (0.5))])\n\ndef load_data_fashion_mnist(batch_size): #@save\n    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n    mnist_train = torchvision.datasets.FashionMNIST(\n        root=\"../data\", train=True, transform=train_augs, download=True)\n    mnist_test = torchvision.datasets.FashionMNIST(\n        root=\"../data\", train=False, transform=test_augs, download=True)\n    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n                            num_workers=4),\n            data.DataLoader(mnist_test, batch_size, shuffle=False,\n                            num_workers=4))\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        \"\"\"定义BasicBlock残差块类\n\n        参数：\n            inplanes (int): 输入的Feature Map的通道数\n            planes (int): 第一个卷积层输出的Feature Map的通道数\n            stride (int, optional): 第一个卷积层的步长\n            downsample (nn.Sequential, optional): 旁路下采样的操作\n        注意：\n            残差块输出的Feature Map的通道数是planes*expansion\n        \"\"\"\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Conv2d(planes, planes // 16, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv2d(planes // 16, planes, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out2 = self.se(out)\n        out = out * out2\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=10, zero_init_residual=False):\n        \"\"\"定义ResNet网络的结构\n\n       参数：\n           block (BasicBlock / Bottleneck): 残差块类型\n           layers (list): 每一个stage的残差块的数目，长度为4\n           num_classes (int): 类别数目\n           zero_init_residual (bool): 若为True则将每个残差块的最后一个BN层初始化为零，\n               这样残差分支从零开始每一个残差分支，每一个残差块表现的就像一个恒等映射，根据\n               https://arxiv.org/abs/1706.02677这可以将模型的性能提升0.2~0.3%\n       \"\"\"\n\n        super(ResNet, self).__init__()\n        self.inplanes = 64  # 第一个残差块的输入通道数\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # Stage1 ~ Stage4\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # GAP\n        self.last = nn.Linear(512 , num_classes)\n\n        # 网络参数初始化\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        \"\"\"定义ResNet的一个Stage的结构\n\n        参数：\n            block (BasicBlock / Bottleneck): 残差块结构\n            plane (int): 残差块中第一个卷积层的输出通道数\n            bloacks (int): 当前Stage中的残差块的数目\n            stride (int): 残差块中第一个卷积层的步长\n        \"\"\"\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.last(x)\n\n        return x\n\nnet = ResNet(BasicBlock, [2, 2, 2, 2])\n# print(model)\n\npretrained_net = torchvision.models.resnet18(pretrained=True)\n# net = pretrained_net\n# print(net)\npretrained_dict = pretrained_net.state_dict()\nmodel_dict = net.state_dict()\npretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n# for k,v in pretrained_dict.items():\n#     if k in model_dict:\n#         print(1)\n#     else:\n#         print(0)\nmodel_dict.update(pretrained_dict)\nnet.load_state_dict(model_dict)\nnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nnet.conv1.weight.data = pretrained_net.conv1.weight[:,:1,:,:]\nprint(net)\n\n# for layer in net.modules():\n#     X = layer(X)\n#     print(layer.__class__.__name__,'output shape: \\t',X.shape)\n# print()\n\n\n# 如果param_group=True，输出层中的模型参数将使⽤⼗倍的学习率\ndef train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=10,\n                      param_group=True):\n    train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)\n\n    devices = d2l.try_all_gpus()\n    loss = nn.CrossEntropyLoss(reduction=\"none\")\n    if param_group:\n        params_1x = [param for name, param in net.named_parameters()\n                     if name not in [\"last.weight\", \"last.bias\"]]\n        trainer = torch.optim.SGD(\n                                [{'params': params_1x},\n                               {'params': net.last.parameters(),\n                                'lr': learning_rate * 10}],\n                              lr=learning_rate, weight_decay=0.001)\n    else:\n        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n                              weight_decay=0.001)\n    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n                   devices)\n\ntrain_fine_tuning(net, 5e-5)\nX = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n\nend = time.perf_counter()\nprint(end-start)","metadata":{},"execution_count":null,"outputs":[]}]}
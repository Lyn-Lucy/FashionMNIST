{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install d2l","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-01T12:46:55.702631Z","iopub.execute_input":"2022-04-01T12:46:55.703305Z","iopub.status.idle":"2022-04-01T12:47:44.809005Z","shell.execute_reply.started":"2022-04-01T12:46:55.70319Z","shell.execute_reply":"2022-04-01T12:47:44.807308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install matplotlib==2.2.3\n!pip install timm","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:16:14.833465Z","iopub.execute_input":"2022-03-30T09:16:14.833741Z","iopub.status.idle":"2022-03-30T09:16:23.319568Z","shell.execute_reply.started":"2022-03-30T09:16:14.833712Z","shell.execute_reply":"2022-03-30T09:16:23.318693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from torch import nn\n# from d2l import torch as d2l\n# net = nn.Sequential(\n#     # 这⾥，我们使⽤⼀个11*11的更⼤窗⼝来捕捉对象。\n#     # 同时，步幅为4，以减少输出的⾼度和宽度。\n#     # 另外，输出通道的数⽬远⼤于LeNet\n#     nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n#     nn.MaxPool2d(kernel_size=3, stride=2),\n#     # 减⼩卷积窗⼝，使⽤填充为2来使得输⼊与输出的⾼和宽⼀致，且增⼤输出通道数\n#     nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n#     nn.MaxPool2d(kernel_size=3, stride=2),\n#     # 使⽤三个连续的卷积层和较⼩的卷积窗⼝。\n#     # 除了最后的卷积层，输出通道的数量进⼀步增加。\n#     # 在前两个卷积层之后，汇聚层不⽤于减少输⼊的⾼度和宽度\n#     nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n#     nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n#     nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n#     nn.MaxPool2d(kernel_size=3, stride=2),\n#     nn.Flatten(),\n#     # 这⾥，全连接层的输出数量是LeNet中的好⼏倍。使⽤dropout层来减轻过拟合\n#     nn.Linear(6400, 4096), nn.ReLU(),nn.Dropout(p=0.5),\n#     nn.Linear(4096, 4096), nn.ReLU(),nn.Dropout(p=0.5),\n#     # 最后是输出层。由于这⾥使⽤Fashion-MNIST，所以⽤类别数为10，⽽⾮论⽂中的1000\n#     nn.Linear(4096, 10))\n\n\n# X = torch.randn(1, 1, 224, 224)\n# for layer in net:\n#     X=layer(X)\n#     print(layer.__class__.__name__,'output shape:\\t',X.shape)\n\n\n# batch_size = 128\n# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n# lr, num_epochs = 0.01, 10\n# d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n# d2l.plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:03:47.48204Z","iopub.execute_input":"2022-03-30T07:03:47.482296Z","iopub.status.idle":"2022-03-30T07:03:47.488119Z","shell.execute_reply.started":"2022-03-30T07:03:47.48226Z","shell.execute_reply":"2022-03-30T07:03:47.487377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom torch import nn\nfrom d2l import torch as d2l\nimport time\n\nstart = time.perf_counter()\n\ntrain_augs = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(224),\n    torchvision.transforms.RandomHorizontalFlip(),\n    torchvision.transforms.ToTensor()])\ntest_augs = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(224),\n    torchvision.transforms.ToTensor()])\n\ndef load_data_fashion_mnist(batch_size): #@save\n    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n    mnist_train = torchvision.datasets.FashionMNIST(\n        root=\"../data\", train=True, transform=train_augs, download=True)\n    mnist_test = torchvision.datasets.FashionMNIST(\n        root=\"../data\", train=False, transform=test_augs, download=True)\n    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n                            num_workers=4),\n            data.DataLoader(mnist_test, batch_size, shuffle=False,\n                            num_workers=4))\n\nfrom timm import create_model as creat\nmodel = creat('vit_small_patch16_224', pretrained=True, num_classes=10)\nnet = nn.Sequential(nn.Conv2d(1,3,kernel_size=1),model)\n# pretrained_net = torchvision.models.resnet18(pretrained=True)\n# pretrained_net.fc = nn.Linear(pretrained_net.fc.in_features, 10)\n# nn.init.xavier_uniform_(pretrained_net.fc.weight);\n# net = pretrained_net\n# net.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n# net.conv1.weight.data = pretrained_net.conv1.weight[:,:1,:,:]\n# print(net.conv1.weight.shape)\n\n\n# 如果param_group=True，输出层中的模型参数将使⽤⼗倍的学习率\ndef train_fine_tuning(net, learning_rate, batch_size=16, num_epochs=10,\n                      param_group=False):\n    train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)\n\n    devices = d2l.try_all_gpus()\n    loss = nn.CrossEntropyLoss(reduction=\"none\")\n    if param_group:\n        params_1x = [param for name, param in net.named_parameters()\n                     if name not in [\"fc.weight\", \"fc.bias\"]]\n        trainer = torch.optim.SGD(\n                                [{'params': params_1x},\n                               {'params': net.fc.parameters(),\n                                'lr': learning_rate * 10}],\n                              lr=learning_rate, weight_decay=0.001)\n    else:\n        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n                              weight_decay=0.001)\n    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n                   devices)\n# print(model)\n# if hasattr(torch.cuda, 'empty_cache'):\n#     torch.cuda.empty_cache()\ntrain_fine_tuning(net, 5e-4)\n\nend = time.perf_counter()\nprint(end-start)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T09:16:26.971889Z","iopub.execute_input":"2022-03-30T09:16:26.972161Z","iopub.status.idle":"2022-03-30T10:37:46.549392Z","shell.execute_reply.started":"2022-03-30T09:16:26.97213Z","shell.execute_reply":"2022-03-30T10:37:46.548404Z"},"trusted":true},"execution_count":null,"outputs":[]}]}